--- TensorRT/demo/Tacotron2/tensorrt/convert_tacotron22onnx.py	2022-03-24 05:20:03.000000000 +0000
+++ convert_tacotron22onnx.py	2022-03-26 15:55:51.168918919 +0000
@@ -18,7 +18,7 @@
 from torch import nn
 from torch.nn import functional as F
 import argparse
-import tensorrt
+# import tensorrt
 
 import sys
 import os
@@ -50,7 +50,7 @@
                         help='Includes the outer decoder loop in the ONNX model. Enabled by default and only supported on TensorRT 8.0 or later.')
     parser.add_argument('--no-loop', dest='loop', action='store_false',
                         help='Excludes outer decoder loop from decoder ONNX model. Default behavior and necessary for TensorRT 7.2 or earlier.')
-    parser.set_defaults(loop=int(tensorrt.__version__[0]) >= 8)
+    parser.set_defaults(loop=True)
 
     return parser
 
@@ -109,7 +109,7 @@
     for linear in self.layers:
         x1 = F.relu(linear(x1))
         x0 = x1[0].unsqueeze(0)
-        mask = torch.le(torch.rand(256, device='cuda').to(x.dtype), 0.5).to(x.dtype)
+        mask = torch.le(torch.rand(256, device='cpu').to(x.dtype), 0.5).to(x.dtype)
         mask = mask.expand(x1.size(0), x1.size(1))
         x1 = x1*mask*2.0
 
@@ -305,12 +305,12 @@
     args.postnet = os.path.join(args.output, args.postnet)
 
     tacotron2 = load_and_setup_model('Tacotron2', parser, args.tacotron2,
-                                     fp16_run=args.fp16, cpu_run=False)
+                                     fp16_run=args.fp16, cpu_run=True)
 
     opset_version = 10
 
     sequences = torch.randint(low=0, high=148, size=(1,50),
-                             dtype=torch.long).cuda()
+                             dtype=torch.long)
     sequence_lengths = torch.IntTensor([sequences.size(1)])
     dummy_input = (sequences, sequence_lengths)
 
@@ -332,10 +332,10 @@
                       })
 
     decoder_iter = DecoderIter(tacotron2)
-    memory = torch.randn((1,sequence_lengths[0],512)).cuda() #encoder_outputs
+    memory = torch.randn((1,sequence_lengths[0],512)) #encoder_outputs
     if args.fp16:
         memory = memory.half()
-    memory_lengths = sequence_lengths.cuda()
+    memory_lengths = sequence_lengths
     # initialize decoder states for dummy_input
     decoder_input = tacotron2.decoder.get_go_frame(memory)
     mask = get_mask_from_lengths(memory_lengths)
@@ -402,7 +402,7 @@
         insert_decoder_loop(args.decoder, decoder_dir, os.path.basename(args.decoder).replace("_iter", ""), args.fp16)
 
     postnet = Postnet(tacotron2)
-    dummy_input = torch.randn((1,80,620)).cuda()
+    dummy_input = torch.randn((1,80,620))
     if args.fp16:
         dummy_input = dummy_input.half()
     torch.onnx.export(postnet, dummy_input, args.postnet,

