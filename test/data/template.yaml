image: yellow.hub.cambricon.com/magicmind/release/x86_64/magicmind:1.6.0-x86_64-ubuntu18.04-py_3_7

report-result:
    stage: .post
    tags: 
        - bj_test_runner
    script:
        - pip install -r test/requirements.txt
        - python test/show_table.py
    artifacts:
        paths:
            - ${CI_PROJECT_DIR}/benchmark.csv

.network_test:
    stage: test
    tags: 
        - bj_test_runner
    retry: 2
    script:
        - echo "export http_proxy=http://proxy.cambricon.com:8080" >> ~/.bashrc
        - echo "export https_proxy=http://proxy.cambricon.com:8080" >> ~/.bashrc
        - source ~/.bashrc
        # pip install 
        - pip install --upgrade pip
        - pip install nvidia-pyindex
        - pip install -r test/requirements.txt
        # start test
        - source test/dataset_env.sh
        - echo "TEST_PROJ_DIR is:" ${TEST_PROJ_DIR}
        - echo "ls -l TEST_PROJ_DIR is as belows:" 
        - ls -l ${TEST_PROJ_DIR}
        - cd ${TEST_PROJ_DIR}
        - mkdir -p data/models

        #  1. The original "/zoo_ci" is declared in /etc/gitlab-runner/config.toml(the correspoinding key is volume)
        #  /etc/gitlab-runner/config.toml is located in the gitlab-runner docker.
        #  2. Usually, you can entery the gitlab-runner docker by run "docker exec -it gitlab-runner /bin/bash" 
        #  + on the machine which actually offers the gitlab-ci/cd service.
        #  3. in /etc/gitlab-runner/config.toml,volumes = ["/cache","/data/AE/modelzoo/:/data/AE/modelzoo/"] means 
        # + gitlab-runner will map "/data/AE/modelzoo/" on the host machine to "/data/AE/modelzoo/" in the gitlab-runner docker.
        #  On the other hand, you need to map "/data/AE/modelzoo/" on the host machine to "/data/AE/modelzoo/" in the docker when creating the gitlab-runner docker.

        - ln -s /data/AE/modelzoo/models/${TEST_PROJ_DIR##*/}/* data/models

        - if [ -f requirement*.txt ]; then pip install -r requirement*.txt -i  https://pypi.tuna.tsinghua.edu.cn/simple/; fi
        #- if [ -f benchmark/perf.sh ]; then bash benchmark/perf.sh; fi
        #- if [ -f benchmark/eval.sh ]; then bash benchmark/eval.sh; fi
        - cat env.sh
        - set +e
        - source env.sh
        - set -e
        - bash run.sh
    artifacts:
        paths:
            - ${TEST_PROJ_DIR}/benchmark/benchmark.csv
            - ${CI_PROJECT_DIR}/test/*.csv
